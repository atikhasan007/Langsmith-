# LangChain OpenAI Chat Summarizer

This project demonstrates how to use **LangChain** with **OpenAI GPT-3.5 Turbo** to summarize text based on a given context. It is built to show a **modular and scalable approach** to question-context-based text generation.

---

## ðŸ›  Project Overview

The project allows users to ask a **question** about a given **context** (text) and receive a **concise answer** generated by GPT-3.5 Turbo. The main components are:

1. **Prompt Template** â€“ Defines how the input is structured before sending it to the model.
2. **Model** â€“ Uses OpenAI's GPT model (`gpt-3.5-turbo`) to generate text.
3. **Output Parser** â€“ Converts the modelâ€™s response into a structured string output.
4. **Environment Variables** â€“ API keys are managed securely using `.env` files.

---

## ðŸ’¡ How LangChain Works in This Project

LangChain is a **framework for building applications with language models**. In this project:

1. **ChatPromptTemplate**:  
   - Creates a **system message** (defines model behavior) and **user message** (contains question & context).  
   - Ensures the AI only uses the given context to answer.

2. **ChatOpenAI**:  
   - This is the OpenAI GPT model interface.  
   - Generates answers based on the structured prompt from LangChain.

3. **StrOutputParser**:  
   - Converts the raw AI response into a clean string so it can be used in applications or printed easily.

4. **Chain**:  
   - We use `prompt | model | output_parser` to form a **pipeline**:  
     - **Input â†’ Prompt â†’ Model â†’ Output Parser â†’ Result**

This makes the project **modular** and easy to extend with new parsers, models, or prompts.

---

## ðŸ§© Project Workflow

1. **Load environment variables** from `.env` file (API keys).
2. **Define system and user messages** using `ChatPromptTemplate`.
3. **Instantiate GPT model** using `ChatOpenAI`.
4. **Connect the components into a chain**.
5. **Invoke the chain** with a question and context.
6. **Receive the summarized answer** as a clean string.

---

